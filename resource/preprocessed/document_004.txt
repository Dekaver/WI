NameError: name 'nltk' is not defined
Quabr
answers we find
Explore tags
Advanced Dictionary for Chrome
1
Translate worlds/phrases
Translation unknown words or phrases by selection text in a small pop-up bubble.
2
Add flashcards
Create cards in one click with the translated words.
3
Build vocabulary lists
Organize flashcards in different themed decks as you wish.
Get Free Extension
Related questions
My VS code interface for Jupyter notebook is very different from the new version
pd.read_csv optimization to reduce the running time
Cannot locate child element Selenium
Get only the IP Adress in Anaconda Powershell
How to add UCB module in jupyter notebook?
TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key. getting error
How to read a text and label each word of it in Python
nltk's regexp_tokenize make it as word_tokenize
Tokenize the words in the columns and create embedding vectors with
NameError: name 'nltk' is not defined
2021-01-03 08:00
midsummer
imported from Stackoverflow
python
anaconda
nltk
I have a Python notebook that used to work before without problems.
Now if I try to run it, I get an "NameError: name 'nltk' is not defined" error.
I'm not sure what has changed since on my system.
I tried so far:
Installing nltk library with pip
Downloading nltk library with python interpreter
pip3 install nltk
Requirement already satisfied: nltk in /Applications/anaconda3/lib/python3.8/site-packages (3.5)
python
>>>import nltk
>>>nltk.download('all')
$ which python python2 python3
/Applications/anaconda3/bin/python
/usr/bin/python2
/Applications/anaconda3/bin/python3
$ which pip pip2 pip3
/Applications/anaconda3/bin/pip
/Applications/anaconda3/bin/pip3
$ python3 --version
Python 3.8.3
$ echo $PATH
/Applications/anaconda3/bin:/Applications/anaconda3/condabin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/usr/X11/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Users/testimo/.rvm/bin:/Users/testimo/.rvm/bin
Is this path wrong? How can I fix it?
Or any other tips? Thank you!
1 answer
answered 2021-01-03 08:05
Eklavya Chandra
This happens with me; but when I use Google Colab or change the directory to where the python files are, this resolves the issue. You should make a new folder (remember not to make a subfolder in the existing ones) and move the files into it.
See also questions close to this topic
My VS code interface for Jupyter notebook is very different from the new version
I recently started learning python for data analysis and decided to use VS code for its new Jupyter notebook feature. I installed the extension for python and jupyter notebook in VS code but for some reason my interface is very different from the one in the new update.
I understand this is a small issue but, It's really throwing me off. Can someone please let me know what I am doing wrong here?
Thank you
pd.read_csv optimization to reduce the running time
My input file is 20GB .txt file, So It faces performance issues when I test run the below code.
pd.read_csv is taking more than 3 hours. Need to optimization in the reading stage.
Sample input file.
007064770000|SODIUM|95 MILLIGRAM|0
007064770000|MULTI|001|0
007064770000|PET STARCH FREE|NOT APPLICABLE|0
007064770000|GRAIN TYPE|FLOUR|0
003980010200|MULTI|001|0
003980010200|DEAL|NON-DEAL|0
003980010200|PRODUCT SIZE|1 COUNT|0
003980010200|BASE SIZE|1 COUNT|0
757582821517|HW APPLIANCES|001|0
757582821516|HW APPLIANCES|001|0
757582821517|PACKAGE GENERAL SHAPE|BOTTLE|0
757582821517|SYND FORM|CREAM|0
757582821517|FORM|CREAM|0
757582821517|TARGET SKIN CONDITION|DRY SKIN|0
003980010205|HW MEDICINE|NON-DEAL|0
003980010205|PRODUCT SIZE|1 COUNT|0
003980010205|BASE SIZE|1 COUNT|0
007064770054|SODIUM|95 MILLIGRAM|0
007064770054|HW SPORTS|001|0
007064770054|PET STARCH FREE|NOT APPLICABLE|0
007064770054|GRAIN TYPE|FLOUR|0
003980010312|HW DIAMETER|1 COUNT|0
003980010312|BASE SIZE|1 COUNT|0
Output file
UPC code HW APPLIANCES HW DIAMETER HW MEDICINE HW SPORTS
0
3980010205
NaN
NaN
NON-DEAL
NaN
1
3980010312
NaN
1 COUNT
NaN
NaN
2
7064770054
NaN
NaN
NaN
001
3
757582821516
001
NaN
NaN
NaN
4
757582821517
001
NaN
NaN
NaN
Existing code
import pandas as pd
import datetime
df = pd.read_csv('sample.txt', sep='|', names=['upc_cd', 'chr_typ', 'chr_vl', 'chr_vl_typ'], engine='python')
df = df[df['chr_typ'].str.contains('HW ')]
df.sort_values('chr_typ')
df = (
df.iloc[:, :-1]
# Remove last Column
.pivot(index=['upc_cd'], columns=['chr_typ'])
.droplevel(0, axis=1)
# Fix Levels and axes names
.rename_axis('UPC code')
.rename_axis(None, axis=1)
.reset_index()
)
print(df)
df.to_csv('output.csv', sep=',', index=None, mode='w', encoding='utf-8')
Please suggest the modification to the code in order to reduce the running time
Cannot locate child element Selenium
Trying to pick out single restaurant elements under the All Restaurants category on this page. https://www.foodpanda.sg/restaurants/new?lat=1.2915902&lng=103.8379066&vertical=restaurants
All li elements have a different class name. The only working xpath, I have been able to figure out is this one.
//ul[@class="vendor-list"]//li[3]
I cannot figure out how to increase this number and get all the restaurants, which is complicated by the fact that there is an infinite scroll as well.
j = 1
def get_rest():
global j
while True:
driver.execute_script("window.scrollBy(0,2525)", "")
time.sleep(5)
var = f'{[j]}'
elems = driver.find_elements_by_xpath(f'//ul[@class="vendor-list"]//li{var})
return elems
Get only the IP Adress in Anaconda Powershell
I want to get the Output of my Ip Address only inside the Anaconda Powershell in Windows.
curl https://ipinfo.io/ip
Gives me the following:
StatusCode
: 200
StatusDescription : OK
Content
: 22.031.123.456
RawContent
: HTTP/...
...
...
...
...
Content-Type: text/html; charset=utf-8
Date: ...
Via:...
Forms
: {}
Headers
: {[..., *], [...], [...],
[..., ...}
Images
: {}
InputFields
: {}
Links
: {}
ParsedHtml
: ...
RawContentLength
: ...
As you can see, "Content" shows my IP Adress. But I can't get the IP-Address without any other informations.
Note: "curl api.ipify.org" and "curl ipinfo.io/ip" and others are doing the exact same thing.
How to add UCB module in jupyter notebook?
I'm trying to add this ucb module (The UCB module contains functions specific to 61A projects at UC Berkeley.) in jupyter notebook but I've no idea how to go about it. Can somebody please provide me with clear cut instructions on how to do this? The following is the link to the module I want to add:
https://inst.eecs.berkeley.edu/~cs61a/sp14/proj/ants/ucb.py.html
Appreciate the help.
TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key. getting error
`from keras import models
modelvgg.layers.pop()
modelvgg = models.Model(inputs=modelvgg.inputs, outputs=modelvgg.layers[-1].output)
show the deep learning model
modelvgg.summary()`
How to read a text and label each word of it in Python
data = ("Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country. Many people have been killed that day.",
{"entities": [(48, 54, 'Category 1'), (77, 81, 'Category 1'), (111, 118, 'Category 2'), (150, 173, 'Category 3')]})
data[1]['entities'][0] = (48, 54, 'Category 1') stands for (start_offset, end_offset, entity).
I want to read each word of data[0] in a sequential manner and tag each word according to data[1] entities. I am expecting to have as final output,
{
'Thousands': 'O',
'of': 'O',
'demonstrators': 'O',
'have': 'O',
'marched': 'O',
'through': 'O',
'London': 'S-1',
'to': 'O',
'protest': 'O',
'the': 'O',
'war': 'O',
'in': 'O',
'Iraq': 'S-1',
'and': 'O'
'demand': 'O',
'the': 'O',
'withdrawal': 'O',
'of': 'O',
'British': 'S-2',
'troops': 'O',
'from': 'O',
'that': 'O',
'country': 'O',
'.': 'O',
'Many': 'O',
'people': 'S-3',
'have': 'B-3',
'been': 'B-3',
'killed': 'E-3',
'that': 'O',
'day': 'O',
'.': 'O'
}
Here, 'O' stands for 'OutOfEntity', 'S' stands for 'Start', 'B' stands for 'Between', and 'E' stands for 'End' and are unique for every given text.
I tried the following:
def ner(data):
entities = {}
offsets = data[1]['entities']
for entity in offsets:
entities[data[0][int(entity[0]):int(entity[1])]] = re.findall('[0-9]+', entity[2])[0]
tags = []
for key, value in entities.items():
entity = key.split()
if len(entity) > 1:
bEntity = entity[1:-1]
tags.append((entity[0], 'S-'+value))
for item in bEntity:
tags.append((item, 'B-'+value))
tags.append((entity[-1], 'E-'+value))
else:
tags.append((entity[0], 'S-'+value))
tokens = nltk.word_tokenize(data[0])
OTokens = [(token, 'O') for token in tokens if token not in [token[0] for token in tags]]
for token in OTokens:
tags.append(token)
return tags
But the above function does not work properly in case I have some words that are the same as those in data[1]['entities'] offsets but not part of the offsets will be ignored instead they should be labeled as 'O'.
nltk's regexp_tokenize make it as word_tokenize
We have nltk.word_tokenize and nltk.tokenize.regexp.regexp_tokenize for word tokenization. word_tokenize doesn't take arguments and splits by white spaces and special characters, while the regexp_tokenize requires a regex expression to define behaviour. I would like to make it the same behaviour as word_tokenize, so set pattern:
from nltk import word_tokenize
from nltk.tokenize.regexp import regexp_tokenize
pattern = ''
text = 'Hey this ( is * some (text)'
tokens1 = word_tokenize(text)
tokens2 = regexp_tokenize(text ,pattern=pattern, gaps=True)
I want tokens1 and tokens2 to be exactly the same. What should I set pattern to to mimic the exact same behaviour of word_tokenize?
Tokenize the words in the columns and create embedding vectors with
i need your help.
For a Neural Network i need to change the words into word_index (e.g. "company" --> 34, "review" --> 21 etc.)
Here is a picture:
My idea:
Iterating through all the keys and changing them to the word_index before creating the dataframe. However, I'm a little bit lost. That was my last try:
"""
for k, v in w2v_my.items():
tk.word_index(k)
--> Raises that object is not callable. Also i know, that changing the keys in the object is not that good.
"""
I really appreciate your help.
EDIT:
The next step would be to transform the arrays for each word into a a cosine similarity vector e.g. company (key) --> changing to the word_index e.g. 43 --> with an array (value) --> changing to the cosine similarity vector e.g. 0.34333554432.
Quabr.com is StackOverFlow proxy site | Sitemap